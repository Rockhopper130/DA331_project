{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType, ArrayType\n",
    "\n",
    "# from pyspark.ml.classification import XGBoostClassifier\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 10:49:50 WARN Utils: Your hostname, NN.local resolves to a loopback address: 127.0.0.1; using 192.168.0.191 instead (on interface en0)\n",
      "23/12/01 10:49:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/01 10:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"XGBoost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"../data/distilbert_imdb2.csv\", header=True, inferSchema=True, sep=',')\n",
    "data = data.withColumnRenamed(\"sentiment\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    elements = x.strip('[]').split(' ')\n",
    "    result = [float(i) for i in elements if i.strip() != '']\n",
    "    return (result) if result else None\n",
    "\n",
    "parse_embedding_udf = udf(lambda x: parser(x), ArrayType(FloatType()))\n",
    "data = data.withColumn(\"parsed_embeddings\", parse_embedding_udf(data[\"embeddings_distilbert\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"embeddings_distilbert\", \"reviews_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# UDF to convert array to Vector\n",
    "vector_udf = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "data = data.withColumn(\"parsed_embeddings_vector\", vector_udf(data[\"parsed_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_cols = data.columns[1:-2] + data.columns[-1:]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier\n",
    "xgboost = SparkXGBClassifier(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"label\",\n",
    "    prediction_col=\"prediction\",\n",
    "    eval_metric=\"logloss\",  # Evaluation metric\n",
    "    max_depth=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=25739Kb max_used=25890Kb free=105332Kb\n",
      " bounds [0x00000001089d8000, 0x000000010a358000, 0x00000001109d8000]\n",
      " total_blobs=10142 nmethods=9179 adapters=874\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "2023-12-01 10:50:28,278 INFO XGBoost-PySpark: _fit Running xgboost-2.0.1 on 1 workers with\n",
      "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'max_depth': 6, 'eval_metric': 'logloss', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "[10:50:51] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "2023-12-01 10:51:25,192 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model = xgboost.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 10:51:51,138 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,745 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,749 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,763 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,765 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,896 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,897 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2023-12-01 10:51:51,904 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8812946012543919\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/pyspark_distilbert_XGB_model\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da331_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
